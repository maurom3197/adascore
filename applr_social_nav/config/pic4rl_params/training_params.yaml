training_params:
# Trainer parameters
    --max-steps: 1000000 # (int) The maximum steps for training. The default is ``int(1e6)``
    --episode-max-steps: 500 # (int) The maximum steps for an episode. The default is ``int(1e3)``
    --n-experiments: 1 # (int) Number of experiments. The default is ``1``
# --show-progress: # Call ``render`` function during training
    # --save-model-interval: 10000 # (int) Interval to save model. The default is ``int(1e4)``
    --save-model-interval: 5000 # (int) Interval to save model. The default is ``int(1e4)``
    --save-summary-interval: 5000 # (int) Interval to save summary. The default is ``int(1e3)``
    # --model-dir: 'src/Models/SFW/SFW_SAC_160kstep_20240306T232749.683208_SAC_' # (str) Directory to restore model. src/Models/SFW/SFW_SAC_160kstep_20240306T232749.683208_SAC_
#    --dir-suffix: ""  # (str) Suffix for directory that stores results.
#    --normalize-obs: # Whether normalize observation
    --logdir: 'src/Results' # (str) Output directory name. The default is ``"results"``
#    --evaluate: # Whether evaluate trained model
    --test-interval: 5000 # (int) Interval to evaluate trained model. The default is ``int(1e4)``
# --show-test-progress: # Call ``render`` function during evaluation.
    --test-episodes: 3 # (int) Number of episodes at test. The default is ``5``
# --save-test-path: # Save trajectories of evaluation.
# --show-test-images: # Show input images to neural networks when an episode finishes
# --save-test-movie: # Save rendering results.
    --use-prioritized-rb: # Use prioritized experience replay
    --use-nstep-rb: # Use Nstep experience replay
    --n-step: 4 # (int) Number of steps for nstep experience reward. The default is ``4``
    --rb-path-save: 'SAC_1M' # Save experience replay buffer in case of save
#    --rb-path-load: 'src/Results/20240301_191712.280195_applr_SAC/SFW_SAC_500K.npz' # Load experience replay buffer in case of restore
    --logging-level: DEBUG # (DEBUG, INFO, WARNING) Choose logging level. The default is ``INFO``
    --policy: 'SAC' # training policy
    --policy_trainer: 'off-policy' # trainer off-policy / on-policy
    --change_goal_and_pose: 1
    --starting_episodes: 80
    --batch-size: 32 # (int) Batch size for training. The default is ``32``.
    --gpu: 0 # (int) GPU id. ``-1`` disables GPU. The default is ``0``.

# For all off-policy algos (DDPG, TD3, SAC)
    --n-warmup: 10000 # (int) Number of warmup steps before training. The default is ``1e4``.
    --memory-capacity: 1000000  # (int) Replay Buffer size. The default is ``1e6``.

# For only SAC
    --alpha: 0.2 # (float) Temperature parameter. The default is ``0.2``.
    --auto-alpha: # Automatic alpha tuning
# For only SAC-AE
#   --stop-q-grad: # Whether stop gradient after convolutional layers at Encoder

# For on-policy algorithms (PPO)
#    --horizon: 2048  # (int) The default is ``2048``.
#    --normalize-adv: # Normalize Advantage. default True
#    --enable-gae: # Enable GAE. Default True
